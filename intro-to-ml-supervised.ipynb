{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impossible-covering",
   "metadata": {},
   "source": [
    "## Intro to Machine Learning Part 2\n",
    "\n",
    "This workshop will guide you through the basics of using supervised machine learning techniques using the __pandas__ and __scikit-learn__ libraries. The focus of this workshop is not to guide you through the specifics of manipulating your data at each step but to provide a high-level overview of what you need to do to create a simple regression or classification model.\n",
    "\n",
    "The slides to the workshop can be found [here](https://tinyurl.com/cads-ml-intro).\n",
    "\n",
    "### Terms You Need to Know\n",
    "- Data Set: a collection of data typically presented in tabular format\n",
    "- Feature: a column in a data set; a measurable quality\n",
    "- Tidy data: structuring data sets in a standard manner; each row represents a single observation\n",
    "- Imputation: the process of replacing missing data with substituted data\n",
    "- Encoding: process of representing data into a different form; commonly used to represent categorical as numerical data for many models like linear regression and logistic regression\n",
    "- Feature scaling: the processing of preparing data to minimize overfitting; use to increase performance of models (i.e. distance-based and gradient descent-based algorithms) by making the values of features (or range) more similar to each other\n",
    "- Standardization: transform feature values to have zero mean and a variance of 1; make data unitless and features comparable to each other\n",
    "- Normalization: transform feaure values to a range of [0, 1] or [-1, 1]; a.k.a. min-max scaling; useful when data doesn't follow normal distribution\n",
    "- Training data set: the subset of a data set used for training a machine learning model\n",
    "- Validation data set: the subset of the training data set used to tune hyperparameters\n",
    "- Testing Data Set: the subset of a data set used to assess performance on a machine learning model after training\n",
    "- Model parameter: a variable which is used to define a model and can be estimated from the data\n",
    "- Model hyperparameter: a variable which is used to tune a model and cannot be estimated from data (i.e. you have to adjust it yourself)\n",
    "- Supervised learning: a machine learning task where the data set has an experimental or known value the model will predict (i.e. the data set is labeled)\n",
    "- Unsupervised learning: a machine learning task where there is no known value the model will predict for in the data set (i.e. the data set is unlabled or untagged); the model will find patterns or groupings in the data set not specified beforehand\n",
    "- Cross Validation: used to ensure the model is robust and is not overfitted; the most common method is K-fold cross validation where the data set is split into subsets and the model is trained and assessed on those subsets\n",
    "- Metric: values that can be used to make a decision (ie. tell you how well your model is performing and how well it is comparing to other models)\n",
    "- Underfitting: occurs when a model performs poorly on the training and other data\n",
    "- Overfitting: occurs when a model fits and performs really well on the training data but performs poorly on other data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-number",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "Here are steps that are commonly taken by data scientists and ML engineers for their machine learning projects. At each step, there are questions to consider when constructing your pipeline. This guide will still be used as a basis, however, we will not go into the specifics at each step. I strongly encourage you to research each step to improve your own pipelines.\n",
    "\n",
    "![Questions to ask for your ML pipeline](pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-election",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "The goal of classification is to predict discrete values. The common classification algorithms are:\n",
    "- Logistic Regression\n",
    "- Naive Bayes Classifier\n",
    "- Support Vector Machines\n",
    "- Decision Trees\n",
    "- K-Nearest Neighbor\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-processor",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-reputation",
   "metadata": {},
   "source": [
    "##### Data Collection\n",
    "\n",
    "- _Where are you getting this data from?_\n",
    "- _Is the entire data set relevant?_\n",
    "\n",
    "<br />\n",
    "\n",
    "https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "\n",
    "> \"This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\"\n",
    "<br /><br />\"The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\"\n",
    "<br /><br />\"Can you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?\"\n",
    "\n",
    "<br />\n",
    "\n",
    "The __pandas__ package can be used to easily manipulate tabular data (i.e. data mungling/wrangling), prepare it for visualization, and run statistical analysis on it. It uses tables, or an object of rows and columns, called _data frames_ to format data. The package contains several functions to read data in various formats like CSV, Excel, SQL, and HTML among many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "diabetes = pd.read_csv(\"diabetes.csv\")\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-subscription",
   "metadata": {},
   "source": [
    "##### Data Preparation\n",
    "\n",
    "- _Is the data \"tidy?\"_\n",
    "- _Can features be dropped or created?_\n",
    "- _What modifications will the data need to train a model on it?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Does my data contain only numerical features? If not, should I encode or remove/drop the categorical features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Is there any duplicated data or rows that need to be dropped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Is there any missing data? If so, should we impute it or drop those features with the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-holmes",
   "metadata": {},
   "source": [
    "The __matplotlib__ and __seaborn__ packages can be used to visualize data in a format that is easily read and understood. Here, we are using matplotlib and seaborn to view the correlation matrix (i.e. a plot which shows correlation or R-value between features). This will aid in assesing whether or not multicolinearity is present in our data set.\n",
    "\n",
    "Why is multicolinearity an issue? Logisitic regression relies on several of the same assumptions as linear regression and assumes each variable for \"X\" is independent and normally distributed. Keeping two features that are colinear will produce unreliable coefficients or bias terms for those features and can cause the model to overfit the data. There are several ways to detect multicolinearity (e.g. VIF, eigenvalues) but we will be using the correlation coefficient or R-values. A strong correlation is likely an indicator multicolinearity is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# 4. Is there multicolinearity present in the data? \n",
    "#    Are there features that are colinear or linearly dependent that we can drop?\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(FILL, annot=True, cmap='Blues')\n",
    "plt.title(\"Feature Correlation Matrix\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-singles",
   "metadata": {},
   "source": [
    "What are the assumptions for logistic regression? Discussing this during the preprocessing or the data preparation step is essential to figure out what additional steps are needed to manipulate our data for best performance in our model. Logistic regression makes the following assumptions:\n",
    "- The dependent variable is either binary or ordinal depending on whether we are aiming for binary logistic regression or ordinal logisitic regression, respectively.\n",
    "- Each observation, or each row, in the data set should be independent from each other.\n",
    "- There is little to low multicolinearity present among the independent variables.\n",
    "- There is linearity of independent variables and log odds, so the independent variables should be linearly related to the log odds. Linearity between the independent variables and the dependent variable is only assumed for linear regression not logistic regression.\n",
    "- There is a large sample size.\n",
    "\n",
    "Because of these assumptions, we may need to either normalize or standardize the data. The goal of normalziation is to change numerical values to a common range. It is useful to use when you do not know the distribution of your features or the distribution of your features is not normal and your features have varying scales and the algorithm doesn't make any assumptions of the data's distribution as in k-Nearest Neighbors and neural networks.\n",
    "\n",
    "The goal of standardization is to be able to compare features with different units by centering the feature with a mean of 0 and a standard deviation of 1. It is useful when your features have varying scales and the algorithm assumes the data's distrubution s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Do the features need to normalized or standardized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.hist(figsize=(11,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# diabetes_X = sc.fit_transform(diabetes.drop('Outcome', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-bargain",
   "metadata": {},
   "source": [
    "##### Model Training\n",
    "\n",
    "- _Which model makes sense for my data and predictions?_\n",
    "- _How can the data set be split to train and test the model without overfitting?_\n",
    "\n",
    "Before a model is trained, data sets are typically split into two subsets: the training data set and the testing data set. The ratio of the split can be 75% training-15% testing, 60% training-40% testing, 80% training-20% testing, or any other ratio that seems suitable based on the size of your data set and what model you are using. You want to use a higher ratio for your training-testing split if the size of your data set is smaller so the model can have as much data as possible. If your data set is not large enough, you can use alternative methods like k-fold cross validation.\n",
    "\n",
    "The train-test prodedure is commonly used if it becomes costly to repeatedly train your model. Often times, the a validation data set is also created from a subset of the training data set and used to fine tune the model's hyperparameters. Using a validation data set and tuning the hyperparameters is outside the scope of this intro workshop, so we will not be needing this.\n",
    "\n",
    "The __scikit-learn__ package has a function that randomly splits the data set into training and testing data sets as specified by the train-test ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Which features do I need to train the model?\n",
    "\n",
    "# 7. At what ratio should I split my training and testing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-samuel",
   "metadata": {},
   "source": [
    "The __scikit-learn__ package has many machine learning models you can use across supervised and unsupervised learning. Here, we will use the package to fit and train a logistic regression model.\n",
    "\n",
    "You can also tune your model's hyperparameters here like change the penalty function or the solver (i.e. optimization algorithm). We do not use any hyperparameters here because it is outside the scope of this workshop. You can look up all the hyperparameters available on the [package documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). The _max_iter_ parameter tells the model the maximum of iterations needed for the algorithm to converge (i.e. when the model's error get as close as possible to zero or the minimum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit training data to model and adjust any hyperparameters\n",
    "\n",
    "# get prediction for a row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-broad",
   "metadata": {},
   "source": [
    "##### Model Evaluation\n",
    "\n",
    "- _Which metrics can you use to assess your model?_\n",
    "- _Is the model's confusion matrix meaningful?_\n",
    "- _Do other models have improved metric scores?_\n",
    "\n",
    "One way to assess classification tasks is to use a confusion matrix. A confusion matrix shows the model's:\n",
    "- True positives (top left): the number of positive observations and classified as positive by the model\n",
    "- True negatives (bottom right): the number of negative observations and classified as negative by the model\n",
    "- False positives (top right): the number of negative observations that are classified as positive by the model\n",
    "- False negatives (bottom left): the number of positive observations that are classified as negative by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. What does the model's confusion matrix tell us?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-stopping",
   "metadata": {},
   "source": [
    "Several metrics exist for you to assess your model's performance. These metrics can differ depending on the task you are trying to achieve, for instance, classification and regression tasks are assessed via different metrics. The common metrics available for classification tasks are:\n",
    "- Accuracy: the percentage of observations classified correctly by the model; (true positive + true negative) / total observations; use if avoiding both false positives and false negatives; is not useful if there are not the same number of positive and negative cases in the data set\n",
    "- Precision: the percentage of positive observations classifed correctly as positive by the model; true positve / (true positive + false positive); use if avoiding false positives is important\n",
    "- Recall or sensitivity: the percentage of observations classified correctly as positive by the model; true positive / (true positive + false negative); use if avoiding false negatives is important\n",
    "- F1-score: a metric that is a balance between precision and recall (2 * (precision * recall) / (precision + recall); use if there is a large number of true negatives and avoiding false positives and false negatives is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. What do common metrics for the model tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your prediction and run metrics on it\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(diabetes_y_test, diabetes_y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-librarian",
   "metadata": {},
   "source": [
    "The AUC-ROC curve is also a plot uses to assess your classification model. The plot shows the false positive rate on the bottom axis and the true positive rate on the left axis. The ROC is a probability curve and the AUC represents the degree or measure of seperability (i.e. how well the model is capable of distinguishing between classes or at predicting 0s as 0s and 1s as 1s). An AUC of 1 is ideal where an AUC of 0 is the worst. An AUC of 0.5 means the model is not able to distinguish between classes whatsoever. We also want the ROC curve to be as close as possible to the point (0 FPR, 1 TPR). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "y_pred_proba = diabetes_model.predict_proba(diabetes_X_test)[::,1]\n",
    "fpr, tpr, _ = roc_curve(diabetes_y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(diabetes_y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=\"data 1, auc=\" + str(auc))\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-utilization",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "The goal of regression is to predict continuous values. The common regression algorithms are:\n",
    "- Linear Regression\n",
    "- Polynomial Regression\n",
    "- Poisson Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-subscription",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-plastic",
   "metadata": {},
   "source": [
    "##### Data Collection\n",
    "\n",
    "- _Where are you getting this data from?_\n",
    "- _Is the entire data set relevant?_\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"housing.csv\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-holocaust",
   "metadata": {},
   "source": [
    "##### Data Preparation\n",
    "\n",
    "- _Is the data \"tidy?\"_\n",
    "- _Can features be dropped or created?_\n",
    "- _What modifications will the data need to train a model on it?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Does my data contain only numerical features? If not, should I encode or remove/drop the categorical features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Is there any duplicated data or rows that need to be dropped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Is there any missing data? If so, should we impute it or drop those features with the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-bulgaria",
   "metadata": {},
   "source": [
    "What are the assumptions for linear regression? Like logistic regression, linear regression also makes several assumptions. Discussing this during the preprocessing or the data preparation step is essential to figure out what additional steps are needed to manipulate our data for best performance in our model. Linear regression makes the following assumptions:\n",
    "- The relationship between the independent variables and the dependent variable is linear.\n",
    "- The independent variables are normally distributed. This can be checked with a histogram or a Q-Q plot. A transformation (e.g. log, or log-log) can fix the issue if a variable/feature is not normally distributed.\n",
    "- There is little to no multicolinearity present among the independent variables. This can be checked with the correlation matrix.\n",
    "- There is little to no autocorrelation in the data. Do not worry if you don't know what this means as it is mostly relevant for time-series data and is outside the scope of this workshop.\n",
    "- The data is homoscedastic, or the residuals (i.e. the error or difference between the actual and predicted value) are cented around a zero mean. This can be checked with a residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Is there multicolinearity present in the data? \n",
    "#    Are there features that are colinear or linearly dependent that we can drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Do the features need to normalized or standardized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-transmission",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# diabetes_X = sc.fit_transform(diabetes.drop('Outcome', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-alcohol",
   "metadata": {},
   "source": [
    "##### Model Training\n",
    "\n",
    "- _Which model makes sense for my data and predictions?_\n",
    "- _How can the data set be split to train and test the model without overfitting?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Which features do I need to train the model?\n",
    "\n",
    "\n",
    "# 7. At what ratio should I split my training and testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit training data to model and adjust any hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-aurora",
   "metadata": {},
   "source": [
    "##### Model Evaluation\n",
    "\n",
    "- _Which metrics can you use to assess your model?_\n",
    "- _Do other models have improved metric scores?_\n",
    "\n",
    "The common metrics available for regression tasks are:\n",
    "- Mean Squared Error (MSE): squared difference between the predicted and actual value\n",
    "- Mean Absolute Error (MAE): average of the absolute difference between the predicted and actual value\n",
    "- R-squared: the variance of the dependent variable explained by the independent variables; essentially is the strength of the relationship between the independent variables in your model and the dependent variables\n",
    "- Adjusted R-squared: the variation explained by only the independent variables that actually affect the dependent variable not by every single independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. What do common metrics for the model tell us?\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "housing_y_hat = housing_model.predict(housing_X_test)\n",
    "print(\"MAE: \", mean_absolute_error(housing_y_test, housing_y_hat))\n",
    "print(\"MSE: \", mean_squared_error(housing_y_test, housing_y_hat))\n",
    "print(\"R2: \", r2_score(housing_y_test, housing_y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
