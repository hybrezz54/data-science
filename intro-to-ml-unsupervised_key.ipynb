{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thorough-chile",
   "metadata": {},
   "source": [
    "## Intro to Machine Learning Part 3\n",
    "\n",
    "This workshop will guide you through the basics of using unsupervised machine learning techniques using the __pandas__ and __scikit-learn__ libraries. The focus of this workshop is not to guide you through the specifics of manipulating your data at each step but to provide a high-level overview of what you need to do to create a simple regression or classification model.\n",
    "\n",
    "The slides to the workshop can be found [here](https://tinyurl.com/cads-ml-intro-2).\n",
    "\n",
    "### Terms You Need to Know\n",
    "- Data Set: a collection of data typically presented in tabular format\n",
    "- Feature: a column in a data set; a measurable quality\n",
    "- Tidy data: structuring data sets in a standard manner; each row represents a single observation\n",
    "- Imputation: the process of replacing missing data with substituted data\n",
    "- Encoding: process of representing data into a different form; commonly used to represent categorical as numerical data for many models like linear regression and logistic regression\n",
    "- Feature scaling: the processing of preparing data to minimize overfitting; use to increase performance of models (i.e. distance-based and gradient descent-based algorithms) by making the values of features (or range) more similar to each other\n",
    "- Standardization: transform feature values to have zero mean and a variance of 1; make data unitless and features comparable to each other\n",
    "- Normalization: transform feaure values to a range of [0, 1] or [-1, 1]; a.k.a. min-max scaling; useful when data doesn't follow normal distribution\n",
    "- Training data set: the subset of a data set used for training a machine learning model\n",
    "- Validation data set: the subset of the training data set used to tune hyperparameters\n",
    "- Testing Data Set: the subset of a data set used to assess performance on a machine learning model after training\n",
    "- Model parameter: a variable which is used to define a model and can be estimated from the data\n",
    "- Model hyperparameter: a variable which is used to tune a model and cannot be estimated from data (i.e. you have to adjust it yourself)\n",
    "- Supervised learning: a machine learning task where the data set has an experimental or known value the model will predict (i.e. the data set is labeled)\n",
    "- Unsupervised learning: a machine learning task where there is no known value the model will predict for in the data set (i.e. the data set is unlabled or untagged); the model will find patterns or groupings in the data set not specified beforehand\n",
    "- Cross Validation: used to ensure the model is robust and is not overfitted; the most common method is K-fold cross validation where the data set is split into subsets and the model is trained and assessed on those subsets\n",
    "- Metric: values that can be used to make a decision (ie. tell you how well your model is performing and how well it is comparing to other models)\n",
    "- Underfitting: occurs when a model performs poorly on the training and other data\n",
    "- Overfitting: occurs when a model fits and performs really well on the training data but performs poorly on other data\n",
    "- Exporatory Data Analysis (EDA): process of investigating and analyze data sets to discover patterns and anomalies via statistical and visual methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-happening",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "Here are steps that are commonly taken by data scientists and ML engineers for their machine learning projects. At each step, there are questions to consider when constructing your pipeline. This guide will still be used as a basis, however, we will not go into the specifics at each step. I strongly encourage you to research each step to improve your own pipelines.\n",
    "\n",
    "![Questions to ask for your ML pipeline](pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-oxide",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clustering\n",
    "\n",
    "The goal of clustering is to determine the grouping in a set of unlabeled data. The common clustering algorithms are:\n",
    "- k-Means Clustering\n",
    "- Hierarchical Clustering\n",
    "- Mixture of Gaussians\n",
    "- t-SNE Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-bidding",
   "metadata": {},
   "source": [
    "#### k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-waste",
   "metadata": {},
   "source": [
    "##### Data Collection\n",
    "\n",
    "- _Where are you getting this data from?_\n",
    "- _Is the entire data set relevant?_\n",
    "\n",
    "<br />\n",
    "\n",
    "https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python\n",
    "\n",
    "> \"This data set is created only for the learning purpose of the customer segmentation concepts, also known as market basket analysis.\"\n",
    "<br /><br />\"You are owing a supermarket mall and through membership cards , you have some basic data about your customers like Customer ID, age, gender, annual income and spending score. Spending Score is something you assign to the customer based on your defined parameters like customer behavior and purchasing data.\"\n",
    "<br /><br />\"You own the mall and want to understand the different customer segments that exist and can be targeted by your marketing team. Can you build a machine learning model to identify the customer segments?\"\n",
    "\n",
    "<br />\n",
    "\n",
    "The __pandas__ package can be used to easily manipulate tabular data (i.e. data mungling/wrangling), prepare it for visualization, and run statistical analysis on it. It uses tables, or an object of rows and columns, called _data frames_ to format data. The package contains several functions to read data in various formats like CSV, Excel, SQL, and HTML among many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "customers = pd.read_csv(\"mall_customers.csv\")\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-noise",
   "metadata": {},
   "source": [
    "##### Data Preparation\n",
    "\n",
    "- _Is the data \"tidy?\"_\n",
    "- _Can features be dropped or created?_\n",
    "- _What modifications will the data need to train a model on it?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Does my data contain only numerical features? If not, should I encode or remove/drop the categorical features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view info about data set\\\n",
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Is there any duplicated data or rows that need to be dropped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Is there any missing data? If so, should we impute it or drop those features with the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-bullet",
   "metadata": {},
   "source": [
    "The __matplotlib__ and __seaborn__ packages can be used to visualize data in a format that is easily read and understood. Here, we are using matplotlib and seaborn to view the histogram and count plot of the features to perform exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. What does the distribution of the features tell us?\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "plt.figure(1 , figsize = (15, 6))\n",
    "n = 0 \n",
    "for x in ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']:\n",
    "    n += 1\n",
    "    plt.subplot(1, 3, n)\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    sns.histplot(customers[x] , bins = 20)\n",
    "    plt.title('Histplot of {}'.format(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (15, 5))\n",
    "sns.countplot(y = 'Gender' , data = customers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the pairplot tell anything?\n",
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the feature correlation matrix show any highly correlated features?\n",
    "# HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-charge",
   "metadata": {},
   "source": [
    "##### Model Training\n",
    "\n",
    "- _Which model makes sense for my data and predictions?_\n",
    "- _How can the data set be split to train and test the model without overfitting?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-scratch",
   "metadata": {},
   "source": [
    "The __scikit-learn__ package has many machine learning models you can use across supervised and unsupervised learning. Here, we will use the package to fit and train a logistic regression model.\n",
    "\n",
    "You can also tune your model's hyperparameters here like change the penalty function or the solver (i.e. optimization algorithm). We do not use any hyperparameters here because it is outside the scope of this workshop. You can look up all the hyperparameters available on the [package documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-senate",
   "metadata": {},
   "source": [
    "###### Segmentation with Age and Spending Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = customers[['Age' , 'Spending Score (1-100)']].values\n",
    "\n",
    "# elbow method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "    km.fit(x)\n",
    "    wcss.append(km.inertia_)\n",
    "    \n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method for Age and Spending Score', fontsize = 20)\n",
    "plt.xlabel('No. of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and train model\n",
    "# HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-shore",
   "metadata": {},
   "source": [
    "###### Segmentation with Annual Income and Spending Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = customers[['Annual Income (k$)', 'Spending Score (1-100)']].values\n",
    "\n",
    "# elbow method\n",
    "# HERE\n",
    "    \n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method for Annual Income and Spending Score', fontsize = 20)\n",
    "plt.xlabel('No. of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and train model\n",
    "# HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-forward",
   "metadata": {},
   "source": [
    "##### Model Evaluation\n",
    "\n",
    "- _How can you visually assess your model?_\n",
    "- _Do other models show a more distinct pattern?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_means = age_score_model.fit_predict(x)\n",
    "plt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 100, c = 'pink', label = 'careful')\n",
    "plt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 100, c = 'orange', label = 'spendthrift')\n",
    "plt.scatter(x[y_means == 2, 0], x[y_means == 2, 1], s = 100, c = 'cyan', label = 'youngtarget')\n",
    "plt.scatter(x[y_means == 3, 0], x[y_means == 3, 1], s = 100, c = 'magenta', label = 'oldtarget')\n",
    "plt.scatter(age_score_model.cluster_centers_[:,0], age_score_model.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid')\n",
    "\n",
    "plt.title('K Means Clustering', fontsize = 20)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_means = income_score_model.fit_predict(x2)\n",
    "plt.scatter(x2[y_means == 0, 0], x2[y_means == 0, 1], s = 100, c = 'pink', label = 'miser')\n",
    "plt.scatter(x2[y_means == 1, 0], x2[y_means == 1, 1], s = 100, c = 'green', label = 'general')\n",
    "plt.scatter(x2[y_means == 2, 0], x2[y_means == 2, 1], s = 100, c = 'cyan', label = 'target')\n",
    "plt.scatter(x2[y_means == 3, 0], x2[y_means == 3, 1], s = 100, c = 'magenta', label = 'spendthrift')\n",
    "plt.scatter(x2[y_means == 4, 0], x2[y_means == 4, 1], s = 100, c = 'orange', label = 'careful')\n",
    "plt.scatter(income_score_model.cluster_centers_[:,0], income_score_model.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid')\n",
    "\n",
    "plt.title('K Means Clustering', fontsize = 20)\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-trouble",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "The goal of dimensionality reduction is to determine the grouping in a set of unlabeled data. The common dimensionality reduction algorithms are:\n",
    "- Principal Component Analysis (PCA)\n",
    "- Latent Discriminant Analysis (LDA)\n",
    "- Kernel PCA\n",
    "- t-SNE\n",
    "\n",
    "We will not cover dimensionality reduction for the portion of this workshop because of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
